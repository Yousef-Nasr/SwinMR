# SwinMR Data Loading Issues - Solutions and Fixes

## Issues Identified

### 1. NaN/Inf Division Error
**Problem**: `RuntimeWarning: invalid value encountered in divide` at line 221 in `dataset_CCsagnpi.py`

**Root Cause**: The normalization code `gt = (gt - gt.min()) / (gt.max() - gt.min())` fails when:
- All values in the data are the same (gt.max() == gt.min(), causing division by zero)
- The data contains NaN or Inf values

**Solution Applied**:
- Added checks for NaN/Inf values before normalization
- Added safe division that handles the case when max == min
- Used `np.nan_to_num()` to replace NaN/Inf with safe values

### 2. DataLoader Worker Process Error
**Problem**: `RuntimeError: torch.cat(): input types can't be cast to the desired output type Byte`

**Root Cause**: When using `num_workers > 0`, different worker processes may return data with inconsistent data types, causing tensor concatenation to fail.

**Solutions Applied**:
1. **Safe Collate Function**: Created `safe_collate_fn()` that:
   - Ensures all tensors are cast to float32 before concatenation
   - Handles None values gracefully
   - Provides fallback dummy data if batch creation fails

2. **Conservative Worker Configuration**:
   - Limit maximum workers to 4 even if config specifies more
   - Force single-threaded mode (workers=0) on Windows
   - Add `persistent_workers=True` when workers > 0

3. **Enhanced Error Handling**:
   - Wrap training loop in try-catch blocks
   - Skip problematic batches instead of crashing
   - Continue training even if individual batches fail

## Files Modified

### 1. `data/dataset_CCsagnpi.py`
- ✅ Added safe normalization in `load_images()`
- ✅ Added NaN/Inf validation in `undersample_kspace()`
- ✅ Enhanced `generate_gaussian_noise()` with bounds checking
- ✅ Added `safe_collate_fn()` function
- ✅ Added `validate_data_files()` diagnostic function

### 2. `main_train_swinmr.py`
- ✅ Import safe collate function
- ✅ Configure DataLoader with safe settings
- ✅ Add data validation before training starts
- ✅ Wrap training loop with exception handling
- ✅ Skip problematic batches instead of crashing

### 3. `diagnose_data_issues.py` (New)
- ✅ Diagnostic tool to test data loading
- ✅ Test different worker configurations
- ✅ Validate data files for common issues
- ✅ Generate safe configuration files

## Recommendations

### For Immediate Use:
1. **Set `num_workers = 0`** in your JSON configuration files:
   ```json
   {
     "datasets": {
       "train": {
         "dataloader_num_workers": 0
       }
     }
   }
   ```

2. **Run the diagnostic tool** before training:
   ```bash
   python diagnose_data_issues.py --config your_config.json --create-safe-config
   ```

### For Production:
1. **Data Preprocessing**: Check your data files for:
   - Constant value arrays (all pixels same value)
   - NaN or Inf values
   - Extreme value ranges

2. **Configuration**: Use the safe configuration template generated by the diagnostic tool

3. **Monitoring**: Watch for warning messages during training that indicate data issues

## Why It Works on Colab vs Other Machines

The differences between Colab and other machines could be due to:

1. **PyTorch Version**: Different versions handle tensor operations differently
2. **NumPy Version**: Newer versions may have stricter type checking
3. **Data Files**: Colab might have different (cleaner) data files
4. **Environment**: Colab's controlled environment vs. local machine differences
5. **Hardware**: Different CPU architectures may handle operations differently

## Testing

Use the diagnostic tool to test your setup:

```bash
# Test current configuration
python diagnose_data_issues.py --config options/SwinMR/example/train_swinmr_CCnpi_G1D30.json

# Create safe configuration
python diagnose_data_issues.py --config your_config.json --create-safe-config

# Test with safe configuration
python main_train_swinmr.py options/SwinMR/example/your_config_safe.json
```

## Emergency Workaround

If you're still experiencing issues:

1. Set `dataloader_num_workers: 0` in all configs
2. Reduce batch size to 1 temporarily
3. Add this at the top of your training script:
   ```python
   import warnings
   warnings.filterwarnings("ignore", category=RuntimeWarning)
   ```

The fixes implemented should resolve both the NaN/Inf division errors and the DataLoader worker issues, making the training more robust across different environments.
